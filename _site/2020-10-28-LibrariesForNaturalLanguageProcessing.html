<h1 id="useful-python-libraries-for-natural-processing">Useful Python libraries For Natural Processing</h1>

<hr />

<h2 id="1-corenlp">1. CoreNLP</h2>
<p><br /></p>

<p>The <code class="highlighter-rouge">CoreNLP library</code> — a product of Stanford University — was built to be a production-ready natural language processing solution, capable of delivering NLP predictions and analyses at scale. CoreNLP is written in Java, but multiple Python packages and APIs are available for it, including a native Python NLP library called <code class="highlighter-rouge">StanfordNLP</code>.</p>

<p>CoreNLP includes a <code class="highlighter-rouge">broad range of language tools</code>—grammar tagging, named entity recognition, parsing, sentiment analysis, and plenty more. It was designed to be human language agnostic, and currently supports Arabic, Chinese, French, German, and Spanish in addition to English (with Russian, Swedish, and Danish support <code class="highlighter-rouge">available from third parties</code>). CoreNLP also includes a <code class="highlighter-rouge">web API server</code>, a convenient way to serve predictions without too much additional work.</p>

<h2 id="2-genism">2. Genism</h2>

<p><br /></p>

<p><code class="highlighter-rouge">Gensim</code> does just two things, but does them exceedingly well. Its focus is statistical semantics—analyzing documents for their structure, then scoring other documents based on their similarity.</p>

<p>Gensim can work with very large bodies of text by streaming documents to its analysis engine and performing <code class="highlighter-rouge">unsupervised learning</code> on them incrementally. It can create multiple types of models, each suited to different scenarios: Word2Vec, Doc2Vec, FastText, and Latent Dirichlet Allocation.</p>

<p><br /></p>

<h2 id="3-nltk">3. NLTK</h2>

<p><br /></p>

<p><code class="highlighter-rouge">The Natural Language Toolkit</code>, or <code class="highlighter-rouge">NLTK</code> for short, is among the best-known and most powerful of the Python natural language processing libraries.<code class="highlighter-rouge">Many corpora (data sets) and trained models</code> are available to use with NLTK out of the box, so you can start experimenting with NLTK right away.</p>

<p><br /></p>

<p>As the documentation states, NLTK provides a wide variety of tools for working with text: “classification, tokenization, stemming, tagging, parsing, and semantic reasoning.” It can also work with <code class="highlighter-rouge">some third-party tools</code> to enhance its functionality.</p>

<h2 id="4-pattern">4. Pattern</h2>
<p><br />
   If all you need to do is scrape a popular website and analyze what you find, reach for <code class="highlighter-rouge">Pattern</code>. This natural language processing library is far smaller and narrower than other libraries covered here, but that also means it’s focused on doing one common job really well.</p>

<p>Pattern comes with built-ins for scraping a number of popular web services and sources (Google, Wikipedia, Twitter, Facebook, generic RSS, etc.), all of which are available as Python modules (e.g., <code class="highlighter-rouge">from pattern.web import Twitter</code>). You don’t have to reinvent the wheels for getting data from those sites, with all of their individual quirks. You can then perform a variety of common NLP operations on the data, such as sentiment analysis.</p>

<h2 id="5-pynlpi">5. PyNLPI</h2>

<p><br /></p>

<p><code class="highlighter-rouge">PyNLPI</code> (pronounced “pineapple”) has only a basic roster of natural language processing functions, but it has some truly useful data-conversion and data-processsing features for NLP data formats.</p>

<p>Most of the NLP functions in PyNLPI are for basic jobs like tokenization or n-gram extraction, along with some statistical functions useful in NLP like Levenshtein distance between strings or Markov chains. Those functions are implemented in pure Python for convenience, so they’re unlikely to have production-level performance.
<br />
<br /></p>

<p><em>Thanks for reading, cheers!</em></p>
